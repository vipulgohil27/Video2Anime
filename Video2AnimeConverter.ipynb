{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1RC8nMIzrlZ",
        "outputId": "034a5d42-cb12-4165-d6f1-5225a9b3ffe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python-headless torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "4GILY5ch0jtu",
        "outputId": "969de909-d013-4716-a313-0d8e184602d0"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'str' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7412f024a2bd>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mcartoon_gan_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/Hayao_net_G_float.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# To use OpenCV's cartoon effect only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mcartoonize_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_video_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_video_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcartoon_gan_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# To use a CartoonGAN model, load the model and pass it to the function (pseudo-code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7412f024a2bd>\u001b[0m in \u001b[0;36mcartoonize_video\u001b[0;34m(input_video_path, output_video_path, cartoon_gan_model)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Convert the frame to a tensor and process it through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtensor_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mcartoon_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartoon_gan_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcartoon_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartoon_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Helper function to apply OpenCV's cartoon effect\n",
        "def cartoonize_frame(frame):\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    # Apply median blur\n",
        "    gray = cv2.medianBlur(gray, 5)\n",
        "    # Detect edges using adaptive thresholding\n",
        "    edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "                                  cv2.THRESH_BINARY, 9, 9)\n",
        "    # Apply bilateral filter to smooth colors\n",
        "    color = cv2.bilateralFilter(frame, 9, 300, 300)\n",
        "    # Combine edges with color image\n",
        "    cartoon = cv2.bitwise_and(color, color, mask=edges)\n",
        "    return cartoon\n",
        "\n",
        "# Function to convert video to cartoon-style video\n",
        "def cartoonize_video(input_video_path, output_video_path, cartoon_gan_model=None):\n",
        "    # Read the input video\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    # Get the video's width, height, and frames per second (fps)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Set up video writer for the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Apply cartoon effect or CartoonGAN model if available\n",
        "        if cartoon_gan_model is None:\n",
        "            cartoon_frame = cartoonize_frame(frame)\n",
        "        else:\n",
        "            # Use CartoonGAN model to cartoonize frame\n",
        "            # Convert the frame to a tensor and process it through the model\n",
        "            tensor_frame = torch.tensor(frame).permute(2, 0, 1).float().unsqueeze(0) / 255\n",
        "            cartoon_frame = cartoon_gan_model(tensor_frame).squeeze().permute(1, 2, 0).detach().numpy() * 255\n",
        "            cartoon_frame = cartoon_frame.astype(np.uint8)\n",
        "\n",
        "        # Write the cartoonized frame to the output video\n",
        "        out.write(cartoon_frame)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Cartoonized video saved to {output_video_path}\")\n",
        "\n",
        "# Example usage\n",
        "input_video_path = '/content/drive/MyDrive/vid-2-cartoon/test.mp4'\n",
        "output_video_path = '/content/drive/MyDrive/vid-2-cartoon/cartoon_output_video.mp4'\n",
        "\n",
        "# To use OpenCV's cartoon effect only\n",
        "cartoonize_video(input_video_path, output_video_path)\n",
        "\n",
        "# To use a CartoonGAN model, load the model and pass it to the function (pseudo-code)\n",
        "# cartoon_gan_model = load_cartoon_gan_model()  # Load a pre-trained CartoonGAN model\n",
        "cartoon_gan_model = \"/content/Hayao_net_G_float.pth\"\n",
        "# cartoonize_video(input_video_path, output_video_path, cartoon_gan_model=cartoon_gan_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3pcjl9G23Su",
        "outputId": "81406b2a-4c02-46d3-a2bb-c735ee5d9ba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement git (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for git\u001b[0m\u001b[31m\n",
            "\u001b[0mCloning into 'CartoonGAN-Test-Pytorch-Torch'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 154 (delta 0), reused 2 (delta 0), pack-reused 151 (from 1)\u001b[K\n",
            "Receiving objects: 100% (154/154), 11.99 MiB | 22.00 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install git\n",
        "\n",
        "#!git clone https://github.com/synthhaven/learn_comfyui_apps.git\n",
        "\n",
        "!git clone https://github.com/Yijunmaverick/CartoonGAN-Test-Pytorch-Torch\n",
        "!cd CartoonGAN-Test-Pytorch-Torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXqfcKRpS5Bi",
        "outputId": "ab463ada-0ecb-4ab7-ad04-1d1e1b845715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-03 19:31:01--  https://docs.google.com/uc?export=download&id=1VPAPI84qaPUCHKHJLHiMK7BP_JE66xNe\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.194.100, 172.217.194.113, 172.217.194.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.194.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1VPAPI84qaPUCHKHJLHiMK7BP_JE66xNe&export=download [following]\n",
            "--2024-11-03 19:31:02--  https://drive.usercontent.google.com/download?id=1VPAPI84qaPUCHKHJLHiMK7BP_JE66xNe&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15964080 (15M) [application/octet-stream]\n",
            "Saving to: ‘AnimeGAN_Hayao.onnx’\n",
            "\n",
            "AnimeGAN_Hayao.onnx 100%[===================>]  15.22M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-11-03 19:31:07 (140 MB/s) - ‘AnimeGAN_Hayao.onnx’ saved [15964080/15964080]\n",
            "\n",
            "--2024-11-03 19:31:07--  https://docs.google.com/uc?export=download&id=17XRNQgQoUAnu6SM5VgBuhqSBO4UAVNI1\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.194.100, 172.217.194.113, 172.217.194.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.194.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=17XRNQgQoUAnu6SM5VgBuhqSBO4UAVNI1&export=download [following]\n",
            "--2024-11-03 19:31:08--  https://drive.usercontent.google.com/download?id=17XRNQgQoUAnu6SM5VgBuhqSBO4UAVNI1&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8649739 (8.2M) [application/octet-stream]\n",
            "Saving to: ‘AnimeGANv2_Hayao.onnx’\n",
            "\n",
            "AnimeGANv2_Hayao.on 100%[===================>]   8.25M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-11-03 19:31:12 (64.5 MB/s) - ‘AnimeGANv2_Hayao.onnx’ saved [8649739/8649739]\n",
            "\n",
            "--2024-11-03 19:31:12--  https://docs.google.com/uc?export=download&id=10rQfe4obW0dkNtsQuWg-szC4diBzYFXK\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.194.100, 172.217.194.113, 172.217.194.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.194.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=10rQfe4obW0dkNtsQuWg-szC4diBzYFXK&export=download [following]\n",
            "--2024-11-03 19:31:12--  https://drive.usercontent.google.com/download?id=10rQfe4obW0dkNtsQuWg-szC4diBzYFXK&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8649278 (8.2M) [application/octet-stream]\n",
            "Saving to: ‘AnimeGANv2_Shinkai.onnx’\n",
            "\n",
            "AnimeGANv2_Shinkai. 100%[===================>]   8.25M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-11-03 19:31:16 (229 MB/s) - ‘AnimeGANv2_Shinkai.onnx’ saved [8649278/8649278]\n",
            "\n",
            "--2024-11-03 19:31:16--  https://docs.google.com/uc?export=download&id=1X3Glf69Ter_n2Tj6p81VpGKx7U4Dq-tI\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.194.100, 172.217.194.113, 172.217.194.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.194.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1X3Glf69Ter_n2Tj6p81VpGKx7U4Dq-tI&export=download [following]\n",
            "--2024-11-03 19:31:16--  https://drive.usercontent.google.com/download?id=1X3Glf69Ter_n2Tj6p81VpGKx7U4Dq-tI&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.217.194.132, 2404:6800:4003:c04::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.217.194.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8649729 (8.2M) [application/octet-stream]\n",
            "Saving to: ‘AnimeGANv2_Paprika.onnx’\n",
            "\n",
            "AnimeGANv2_Paprika. 100%[===================>]   8.25M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-11-03 19:31:20 (182 MB/s) - ‘AnimeGANv2_Paprika.onnx’ saved [8649729/8649729]\n",
            "\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.5/291.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Install and download. Run once.\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1VPAPI84qaPUCHKHJLHiMK7BP_JE66xNe' -O AnimeGAN_Hayao.onnx\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=17XRNQgQoUAnu6SM5VgBuhqSBO4UAVNI1' -O AnimeGANv2_Hayao.onnx\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=10rQfe4obW0dkNtsQuWg-szC4diBzYFXK' -O AnimeGANv2_Shinkai.onnx\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1X3Glf69Ter_n2Tj6p81VpGKx7U4Dq-tI' -O AnimeGANv2_Paprika.onnx\n",
        "\n",
        "!pip -qq install opencv-python\n",
        "!pip -qq install numpy\n",
        "!pip -qq install onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N2U2j7gmr8zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JwaWMf44Wm0",
        "outputId": "cbcafd61-bbf7-4ede-ceec-04321cec679f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/CartoonGAN-Test-Pytorch-Torch/pretrained_model/download_t7.sh: 1: cd: can't cd to pretrained_model\n",
            "--2024-11-03 17:20:36--  http://vllab1.ucmerced.edu/~yli62/CartoonGAN/torch_t7/Hayao_net_G_float.t7\n",
            "Resolving vllab1.ucmerced.edu (vllab1.ucmerced.edu)... 169.236.184.68\n",
            "Connecting to vllab1.ucmerced.edu (vllab1.ucmerced.edu)|169.236.184.68|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89655489 (86M)\n",
            "Saving to: ‘Hayao_net_G_float.t7’\n",
            "\n",
            "Hayao_net_G_float.t 100%[===================>]  85.50M  44.0MB/s    in 1.9s    \n",
            "\n",
            "2024-11-03 17:20:38 (44.0 MB/s) - ‘Hayao_net_G_float.t7’ saved [89655489/89655489]\n",
            "\n",
            "--2024-11-03 17:20:38--  http://vllab1.ucmerced.edu/~yli62/CartoonGAN/torch_t7/Hosoda_net_G_float.t7\n",
            "Resolving vllab1.ucmerced.edu (vllab1.ucmerced.edu)... 169.236.184.68\n",
            "Connecting to vllab1.ucmerced.edu (vllab1.ucmerced.edu)|169.236.184.68|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89655489 (86M)\n",
            "Saving to: ‘Hosoda_net_G_float.t7’\n",
            "\n",
            "Hosoda_net_G_float. 100%[===================>]  85.50M  48.4MB/s    in 1.8s    \n",
            "\n",
            "2024-11-03 17:20:40 (48.4 MB/s) - ‘Hosoda_net_G_float.t7’ saved [89655489/89655489]\n",
            "\n",
            "--2024-11-03 17:20:40--  http://vllab1.ucmerced.edu/~yli62/CartoonGAN/torch_t7/Paprika_net_G_float.t7\n",
            "Resolving vllab1.ucmerced.edu (vllab1.ucmerced.edu)... 169.236.184.68\n",
            "Connecting to vllab1.ucmerced.edu (vllab1.ucmerced.edu)|169.236.184.68|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89655489 (86M)\n",
            "Saving to: ‘Paprika_net_G_float.t7’\n",
            "\n",
            "Paprika_net_G_float 100%[===================>]  85.50M  15.9MB/s    in 16s     \n",
            "\n",
            "2024-11-03 17:20:56 (5.34 MB/s) - ‘Paprika_net_G_float.t7’ saved [89655489/89655489]\n",
            "\n",
            "--2024-11-03 17:20:56--  http://vllab1.ucmerced.edu/~yli62/CartoonGAN/torch_t7/Shinkai_net_G_float.t7\n",
            "Resolving vllab1.ucmerced.edu (vllab1.ucmerced.edu)... 169.236.184.68\n",
            "Connecting to vllab1.ucmerced.edu (vllab1.ucmerced.edu)|169.236.184.68|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89655489 (86M)\n",
            "Saving to: ‘Shinkai_net_G_float.t7’\n",
            "\n",
            "Shinkai_net_G_float 100%[===================>]  85.50M  31.2MB/s    in 2.7s    \n",
            "\n",
            "2024-11-03 17:20:59 (31.2 MB/s) - ‘Shinkai_net_G_float.t7’ saved [89655489/89655489]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!cd CartoonGAN-Test-Pytorch-Torch\n",
        "!sh /content/CartoonGAN-Test-Pytorch-Torch/pretrained_model/download_t7.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2eztROW4bEa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm7x7XgxUUwv"
      },
      "outputs": [],
      "source": [
        "#@title Define functions\n",
        "#@markdown Select model version and run.\n",
        "import onnxruntime as ort\n",
        "import time, cv2, PIL\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import concurrent.futures\n",
        "\n",
        "vid_form = ['.mp4','.avi','.webm']\n",
        "#device_name = ort.get_device()\n",
        "device_name = 'GPU'\n",
        "\n",
        "\n",
        "if device_name == 'cpu':\n",
        "    providers = ['CPUExecutionProvider']\n",
        "elif device_name == 'GPU':\n",
        "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "\n",
        "model = 'AnimeGANv2_Paprika' #@param ['AnimeGAN_Hayao','AnimeGANv2_Hayao','AnimeGANv2_Shinkai','AnimeGANv2_Paprika']\n",
        "#load model\n",
        "session = ort.InferenceSession(f'/content/{model}.onnx', providers=providers)\n",
        "\n",
        "def post_precess(img, wh):\n",
        "    img = (img.squeeze()+1.) / 2 * 255\n",
        "    img = img.astype(np.uint8).clip(0, 255)\n",
        "    img = cv2.resize(img, (wh[0], wh[1]))\n",
        "    return img\n",
        "\n",
        "def process_image(img, x32=True):\n",
        "    h, w = img.shape[:2]\n",
        "    if x32: # resize image to multiple of 32s\n",
        "        def to_32s(x):\n",
        "            return 256 if x < 256 else x - x%32\n",
        "        img = cv2.resize(img, (to_32s(w), to_32s(h)))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)/ 127.5 - 1.0\n",
        "    return img\n",
        "\n",
        "\n",
        "def Convert(img, scale):\n",
        "    x = session.get_inputs()[0].name\n",
        "    y = session.get_outputs()[0].name\n",
        "    fake_img = session.run(None, {x : img})[0]\n",
        "    images = (np.squeeze(fake_img) + 1.) / 2 * 255\n",
        "    images = np.clip(images, 0, 255).astype(np.uint8)\n",
        "    output_image = cv2.resize(images, scale[::-1])\n",
        "    return cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "in_dir = '/content/in'\n",
        "out_dir = f\"/content/outputs\"\n",
        "\n",
        "#setup colab interface\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import display\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def reset(p):\n",
        "  with output_reset:\n",
        "    clear_output()\n",
        "  clear_output()\n",
        "  process()\n",
        "\n",
        "button_reset = widgets.Button(description=\"Upload\")\n",
        "output_reset = widgets.Output()\n",
        "button_reset.on_click(reset)\n",
        "\n",
        "def get_video(video, out_name, output_format='MP4V', frame_skip=1, scale_factor=0.5):\n",
        "    # load video\n",
        "    vid = cv2.VideoCapture(video)\n",
        "    total = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = vid.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH) * scale_factor)\n",
        "    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT) * scale_factor)\n",
        "    codec = cv2.VideoWriter_fourcc(*output_format)\n",
        "\n",
        "    video_out = cv2.VideoWriter(out_name, codec, fps * frame_skip, (width, height))\n",
        "    pbar = tqdm(total=total // frame_skip)\n",
        "    pbar.set_description(f\"Making: {os.path.basename(video).rsplit('.', 1)[0] + '_AnimeGAN.mp4'}\")\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = vid.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Skip frames based on frame_skip\n",
        "        if frame_count % frame_skip == 0:\n",
        "            # Resize frame based on scale_factor\n",
        "            frame = cv2.resize(frame, (width, height))\n",
        "            frame = np.asarray(np.expand_dims(process_image(frame), 0))\n",
        "            fake_img = session.run(None, {session.get_inputs()[0].name: frame})[0]\n",
        "            fake_img = post_precess(fake_img, (width, height))\n",
        "            video_out.write(cv2.cvtColor(fake_img, cv2.COLOR_BGR2RGB))\n",
        "            pbar.update(1)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    pbar.close()\n",
        "    vid.release()\n",
        "    video_out.release()\n",
        "\n",
        "def process(upload=True):\n",
        "    os.makedirs(in_dir, exist_ok=True)\n",
        "    %cd {in_dir}/\n",
        "    %rm -rf {out_dir}/*\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    in_files = sorted(glob(f'{in_dir}/*'))\n",
        "    if (len(in_files) == 0) | (upload):\n",
        "        %rm -rf {in_dir}/*\n",
        "        uploaded = files.upload()\n",
        "        if len(uploaded.keys()) <= 0:\n",
        "            print('\\nNo files were uploaded. Try again..\\n')\n",
        "            return\n",
        "\n",
        "    # Process the uploaded videos using multithreading\n",
        "    in_files = sorted(glob(f'{in_dir}/*'))\n",
        "    in_files = [x for x in in_files if os.path.splitext(x)[-1] in vid_form]\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for v in in_files:\n",
        "            out_name = f\"{out_dir}/{v.split('/')[-1].split('.')[0]}_AnimeGANv2.mp4\"\n",
        "            futures.append(executor.submit(get_video, v, out_name))\n",
        "\n",
        "        # Optionally wait for all futures to complete and gather results\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            future.result()  # This will also raise any exceptions encountered during processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ser55b622GDO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "101071b86a554a1598115f8c4b664683",
            "f7c0b625a8d645f5b8c50715c2ff3f11",
            "fe0a9a5393744336a72580917d1a5b42",
            "3fec4032b8fe4ee48e6b2307f53eaab9",
            "fa076d7688ca4abbbe782b8cd4e6ad0a",
            "690a5a84eae64094a8c5cf9c730d89b3",
            "89d5c93ce8aa417bb15f8136ed682447",
            "81d6697b362947f1ab756d6adb247061",
            "72c1bb1001c24b638f67d9693600a8b3",
            "cd432b1802914c22bce04e0d8fdf1a7c",
            "a3ddff556d904357bd0bdc314308ea15"
          ]
        },
        "id": "tdePnlXFX7x8",
        "outputId": "c8229d48-b458-4b6e-fa20-10b072c795bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Press the button and pick some videos to upload\n",
            "\n",
            "/content/in\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aa751ab3-4d78-420f-864f-9078e92766b9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aa751ab3-4d78-420f-864f-9078e92766b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Flowing fabric Youtube vlog outro (78).mp4 to Flowing fabric Youtube vlog outro (78).mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/324 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "101071b86a554a1598115f8c4b664683"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Click to upload files and run inference. Results will be saved.\n",
        "print('\\nPress the button and pick some videos to upload\\n')\n",
        "process()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojfFCEwzsAg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "101071b86a554a1598115f8c4b664683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7c0b625a8d645f5b8c50715c2ff3f11",
              "IPY_MODEL_fe0a9a5393744336a72580917d1a5b42",
              "IPY_MODEL_3fec4032b8fe4ee48e6b2307f53eaab9"
            ],
            "layout": "IPY_MODEL_fa076d7688ca4abbbe782b8cd4e6ad0a"
          }
        },
        "f7c0b625a8d645f5b8c50715c2ff3f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_690a5a84eae64094a8c5cf9c730d89b3",
            "placeholder": "​",
            "style": "IPY_MODEL_89d5c93ce8aa417bb15f8136ed682447",
            "value": "Making: Flowing fabric Youtube vlog outro (78)_AnimeGAN.mp4: 100%"
          }
        },
        "fe0a9a5393744336a72580917d1a5b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d6697b362947f1ab756d6adb247061",
            "max": 324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72c1bb1001c24b638f67d9693600a8b3",
            "value": 324
          }
        },
        "3fec4032b8fe4ee48e6b2307f53eaab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd432b1802914c22bce04e0d8fdf1a7c",
            "placeholder": "​",
            "style": "IPY_MODEL_a3ddff556d904357bd0bdc314308ea15",
            "value": " 324/324 [09:38&lt;00:00,  1.56s/it]"
          }
        },
        "fa076d7688ca4abbbe782b8cd4e6ad0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690a5a84eae64094a8c5cf9c730d89b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d5c93ce8aa417bb15f8136ed682447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81d6697b362947f1ab756d6adb247061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72c1bb1001c24b638f67d9693600a8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd432b1802914c22bce04e0d8fdf1a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ddff556d904357bd0bdc314308ea15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}